checkpointing:
  run_name: local_relora
  save_every_n_steps: 10

  save_to_hf: true
  hf_checkpoint:
    repo_id: relora_pico_test
    collection_slug: yuvalw/pico-relora-6766e4e726eb8811626915c0

  training:
    auto_resume: false
  
  learning_dynamics:
    layer_suffixes: []

model:
  n_layers: 2
  d_model: 96
  activation_hidden_dim: 384
  relora:
    target_modules: 
      - attention
      - swiglu
    reset_frequency: 10
    r: 8
    trainable_scaling: true

data:
  dataset: pico-lm/pretokenized-paloma-tinsy
  dataloader:
    batch_size: 2
    max_seq_len: 10
  
monitoring:
  logging:
    log_every_n_steps: 1
  save_to_wandb: true
  wandb:
    entity: pico-lm
    project: pico-relora

training:
  fabric: 
    accelerator: mps
    strategy: auto
    num_nodes: 1
    num_devices: 1

  max_steps: 50

  optimization:
    lr_scheduler: relora_jagged_cosine
    lr: 0.001
    lr_warmup_steps: 5
    min_lr_ratio: 0.1
    restart_warmup_steps: 5

    gradient_accumulation_steps: 1

evaluation:
  metrics: 
    - blimp
  paloma:
    batch_size: 2
  blimp:
    batch_size: 4
    samples_per_set: null