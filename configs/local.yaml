checkpointing:
  run_name: local
  save_every_n_steps: 10

  save_to_hf: false 
  training:
    auto_resume: false
  
  learning_dynamics:
    layer_suffixes: []

model:
  n_layers: 2
  d_model: 96
  activation_hidden_dim: 384

data:
  dataset: pico-lm/pretokenized-paloma-tinsy
  dataloader:
    batch_size: 32
    max_seq_len: 10
  
monitoring:
  logging:
    log_every_n_steps: 10
  save_to_wandb: false

training:
  fabric: 
    accelerator: mps
    strategy: auto

  learning_rate: 0.001
  max_steps: 10

  optimization:
    lr: 0.001
    lr_warmup_steps: 10

    gradient_accumulation_steps: 16

evaluation:
  metrics: null
  # paloma:
  #   batch_size: 2